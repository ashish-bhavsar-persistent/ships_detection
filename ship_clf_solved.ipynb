{
 "metadata": {
  "name": "",
  "signature": "sha256:003e608223636e4703caff8999fd90e5ffb13d2d2fb7f040e1dc7f792f6bc1c7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Histograma de Gradientes y Maquina de Soporte Vectoriales"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El objetivo de este script ser\u00e1 entrenar una maquina de soporte vectoriales haciendo uso del dataset contenido en el archivo comprimido de nombre dataset.zip. Para ello el primer paso corresponde en descomprimir dicho archivo por medio de la librer\u00eda zipfile."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Decompress dataset and testing images from dataset.zip to obtain folders ships_dataset and scenes\n",
      "import zipfile\n",
      "\n",
      "zip_ref = zipfile.ZipFile(\"dataset.zip\", 'r')\n",
      "zip_ref.extractall()\n",
      "zip_ref.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Haciendo uso de la librer\u00eda del sistema operativo \"os\" se debe crear una lista de python que contenga el path de cada uno de las imagenes de la recien creada carpeta ships_dataset para posteriormente ser le\u00eddos usando OpenCV."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Look for all the JPG files and append them into a python list\n",
      "import os\n",
      "\n",
      "path = 'ships_dataset'\n",
      "img_files = [(os.path.join(root, name))\n",
      "    for root, dirs, files in os.walk(path)\n",
      "    for name in files if name.endswith((\".jpg\"))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El dataset contiene imagenes correspondiente a barcos con un 1 al final del nombre de su archivo y un 0 en caso contrario lo cual usaremos para crear las respectivas etiquetas de nuestro clasificador."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![positive](screenshots/classes.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Definimos el descriptor HOG con los parametros descritos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np \n",
      "\n",
      "# HOG parametrization\n",
      "winSize = (32,32)\n",
      "blockSize = (16,16)\n",
      "blockStride = (8,8)\n",
      "cellSize = (8,8)\n",
      "nbins = 9\n",
      "derivAperture = 1\n",
      "winSigma = -1.\n",
      "histogramNormType = 0\n",
      "L2HysThreshold = 0.2\n",
      "gammaCorrection = 1\n",
      "nlevels = 64\n",
      "useSignedGradients = True\n",
      "\n",
      "# Define HOG descriptor \n",
      "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,\n",
      "\tcellSize,nbins,derivAperture,winSigma,histogramNormType\n",
      "\t,L2HysThreshold,gammaCorrection,nlevels, useSignedGradients)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Leemos secuencialmente las imagenes, obtenemos su descriptor HOG y su clase (Barco: 1, No-Barco: 0) del nombre de la imagen. Tanto los descriptores, como sus clases deben quedar en un arreglo de numpy que podamos usar como entrada al clasificador SVM de SKLearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Retrieve information and image patches from XML\n",
      "features = np.zeros((1,324),np.float32)\n",
      "labels = np.zeros(1,np.int64)\n",
      "for i in img_files:\n",
      "\t# Read images\n",
      "\timg = cv2.imread(i)\n",
      "\t# Resize the image to winSize so HoG descriptor can be extracted\n",
      "\tresized_img = cv2.resize(img, winSize)\n",
      "\t# Compute HOG descriptor ans stack them as features\n",
      "\tdescriptor = np.transpose(hog.compute(resized_img))\n",
      "\t# Stack the descriptors in Numpy array\n",
      "\tfeatures = np.vstack((features, descriptor))\n",
      "\t# Stack the labels\n",
      "\tlabels = np.vstack((labels, int(i[-5])))\n",
      "\n",
      "features = np.delete(features, (0), axis=0)\n",
      "labels = np.delete(labels, (0), axis=0).ravel()\n",
      "\n",
      "# Check dimensions of the respective Numpy arrays\n",
      "print 'Features shape', features.shape\n",
      "print 'Labels shape', labels.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usando la funci\u00f3n train_test_split() de SKLearn partimos el dataset en 80% para train y 20% para test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Split data for training and testing\n",
      "X_train, X_test, y_train, y_test = train_test_split(features, \n",
      "                                                    labels, \n",
      "                                                    test_size=0.2, \n",
      "                                                    random_state=42)\n",
      "# Check dimensions of the resulting Numpy arrays\n",
      "print 'X_train: ', X_train.shape, 'y_train', y_train.shape\n",
      "print 'X_test: ', X_test.shape, 'X_test: ', y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Entrenamos una Maquina de Soporte Vectoriales usando SKLearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define Support Vectors Machine Classifier\n",
      "clf = svm.SVC()\n",
      "# Fit with training data and labels\n",
      "clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Medimos la exactitud de nuestro modelo usando el test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict with the trained classifier for the testing set\n",
      "y_pred = clf.predict(X_test)\n",
      "print 'Accuracy: ', accuracy_score(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obtenemos la precisi\u00f3n, exhaustividad y F1 con la funci\u00f3n classification_report()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Classification report:'\n",
      "print classification_report(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Guardamos el clasificador usando joblib para obtener un archivo de nombre ship_hog_svm_clf.pkl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save classifier\n",
      "joblib.dump(clf, 'ship_hog_svm_clf.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}